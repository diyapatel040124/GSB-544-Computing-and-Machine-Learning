---
title: "Lab 8: Cannabis Types"
author: Diya Patel
format: 
    html:
        toc: true 
        code-fold: true 
        embed-resources: true
echo: true 
---

```{python}
import pandas as pd 
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.compose import ColumnTransformer
```

```{python}
df = pd.read_csv('cannabis_full.csv')
df = df.dropna()
df.head()

```

Part One: Binary Classification 
Create a dataset that is limited to only the Sativa and Indica type cannabis strains. 
Create a final best model for each of the four new model types:
LDA, QDA, SVC, and SVM. 
For each, you should: 
- Choose a metric you will use to select your model, and briefly justify your choice. 
- Find the best model for predicting the Type variable. Don't forget to tune any hyperparameters. 
- Report the cross-validated metric.
- Fit the final model.
- Output a confusion matrix 

Q1: LDA
```{python}
df_types = df[df["Type"].isin(["sativa", "indica"])]
df_types

# define the feature set and target variable 
X = df_types.drop("Type", axis = 1)
y = df_types["Type"]

# create training splits
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.2, random_state = 42, stratify = y
)

num_cols = X.select_dtypes(include=[np.number]).columns
cat_cols = X.select_dtypes(exclude=[np.number]).columns

ct = ColumnTransformer(
    transformers = [
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols)
    ]
)
```
```{python}
lda_pipe = Pipeline([
    ("ct", ct),
    ("lda", LDA())
])

param_grid = [
    {"lda__solver": ["lsqr", "eigen"], "lda__shrinkage": ["auto", 0.1, 0.3, 0.5, 0.7, 0.9]},
]

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

lda_grid = GridSearchCV(
    lda_pipe, 
    param_grid=param_grid,
    scoring={"f1_macro": "f1_macro", "accuracy": "accuracy"},
    refit = "f1_macro",
    cv=cv,
    n_jobs=-1
)

lda_grid.fit(X, y)

print("Best params:", lda_grid.best_params_)
print("CV best macro-F1: {:.3f}".format(lda_grid.best_score_))

y_pred = lda_grid.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

cm = confusion_matrix(y_test, y_pred, labels=["indica", "sativa"])
print("\nConfusion Matrix:\n", cm)
```
Best params: {'lda__shrinkage': 'auto', 'lda__solver': 'lsqr'}
Because the dataset was one-hot-encoded into columns, the best solver was lsqr which supported the shrinkage, and shrinkage helps stabilize LDA when feature correlations are too high. 

CV best macro-F1: 0.857

Classification Report:
               precision    recall  f1-score   support

      indica      0.939     0.932     0.935       132
      sativa      0.892     0.902     0.897        82

    accuracy                          0.921       214
   macro avg      0.915     0.917     0.916       214
weighted avg      0.921     0.921     0.921       214

With a score of 0.916, the f1-score indicated the model performed well and was stable

Indica had a higher precision, but Sativa had a higher recall which means the model was better at predicting actual Sativa strains. 

Confusion Matrix:
 [[123   9] --> out of 132 Indica strains, 123 are classified as Indica 
 [  8  74]] --> out of 82 Sativa strains, 74 are classified as Sativa and 8 were misclassified

Metric Selection: F1-Score 
For LDA, I used the macro F1-Score to find the average of the F1-scores across both of the binary classes which will weigh them equally. F1 makes sure that precision and recall are both balanced which can come up when misclassifying either Sativa or Indica strains. F1 also treats both binary classes equally even when the class sizes are not equal. Macro F1 is the most equal and fair metric for comparing the hyperparameter values.



Q2: QDA
```{python}
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA

qda_pipe = Pipeline([
    ("ct", ct),
    ("qda", QDA())
])

param_grid = {
    "qda__reg_param": [0.01, 0.05, 0.1, 0.2, 0.5]
}

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

qda_grid = GridSearchCV(
    qda_pipe, 
    param_grid=param_grid, 
    scoring="f1_macro", 
    cv=cv, 
    n_jobs=-1
)

qda_grid.fit(X_train, y_train)

print("Best params:", qda_grid.best_params_)
print("Best CV macro-F1: {:.3f}".format(qda_grid.best_score_))

# 9) Evaluate on test
y_pred = qda_grid.predict(X_test)
print("\nClassification report:\n", classification_report(y_test, y_pred, digits=3))

cm = confusion_matrix(y_test, y_pred, labels=["indica", "sativa"])
print("\nConfusion Matrix:\n", cm)
```
Best params: {'qda__reg_param': 0.5}
Grid search chose 0.5 regularization which is a large amount. This means that QDA estimated a separate covariance matrix for each class and with many dummy variables the covariance matrices can be unstable. 

Best CV macro-F1: 0.396
This F1-Score is much lower, and shows that QDA performed poorly in comparison to LDA, so it must not be a good fit for the dataset. 

Classification report:
               precision    recall  f1-score   support

      indica      0.624     0.992     0.766       132
      sativa      0.750     0.037     0.070        82

    accuracy                          0.626       214
   macro avg      0.687     0.515     0.418       214
weighted avg      0.672     0.626     0.499       214

The model nearly always predicted Indica correctly given the high recall value but never correcly identified Sativa with such a low recall score. The F1-Score is low because the performance is unbalanced between the two classes.

Confusion Matrix:
 [[131   1] --> out of 132 Indica strains, 131 are classified as Indica (QDA is biased here)
 [ 79   3]] --> out of 82 Sativa strains, only 3 were classified as Sativa and 79 misclassified

Metric Selection: For QDA, I used the same preprocessing pipeline as in LDA because QDA is a variant on LDA. The predictors were standardized and one-hot encoded. After tuning the hyperparameters I used the F1-Score as the selection metric where Macro F1 averages the F1-scores of Indica and Sativa to treat both binary classes equally and balancing precision and recall and not prioritize either strain. 

QDA is not the ideal linear model for binary classification for the entire dataset because it heavily favored Indica and hardly predicts Sativa. The macro F1-Score is much lower for QDA than in LDA.

Q3: SVC 
```{python}
from sklearn.svm import SVC
svc_pipe = Pipeline([
    ("ct", ct),
    ("svc", SVC())
])

param_grid = {
    "svc__kernel": ["poly"],
    "svc__degree": [2, 3, 4],
    "svc__C": [0.1, 1, 10],
    "svc__gamma": ["scale", "auto"]
}

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

svc_grid = GridSearchCV(
    svc_pipe, 
    param_grid=param_grid, 
    scoring="f1_macro", 
    cv=cv, 
    n_jobs = -1
)

svc_grid.fit(X_train, y_train)

# 9) Results
print("Best params:", svc_grid.best_params_)
print("Best CV macro-F1: {:.3f}".format(svc_grid.best_score_))

# 10) Evaluate on test set
y_pred = svc_grid.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

cm = confusion_matrix(y_test, y_pred, labels=["indica", "sativa"])
print("\nConfusion Matrix:\n", cm)
```
Best params: {'svc__C': 10, 'svc__degree': 3, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}

A 3rd degree polynomial gave the best nonlinear boundary between Sativa and Indica, and a higher C value reduced regularization and fit the model more tightly. The model seemed to need a more complex nonlinear to separate the two strains well. 

Best CV macro-F1: 0.810
The F1-Score means the model performed well across both classes during cross-validation.

Classification Report:
               precision    recall  f1-score   support

      indica      0.834     0.917     0.874       132
      sativa      0.841     0.707     0.768        82

    accuracy                          0.836       214
   macro avg      0.838     0.812     0.821       214
weighted avg      0.837     0.836     0.833       214

The model correctly predicted 92% of Indica and 71% of Sativa. Both had high f1-scoring. 

Confusion Matrix:
 [[121  11] --> Out of 132 predictions, 121 were correct and 11 were misclassified for Indica 
 [ 24  58]] --> out of 82 predictions 58 were correct and 24 were incorrect 
Model was more accurate in predicting Indica strains and Sativas were more misclassified. 


Q4: SVM
```{python}
from sklearn import svm
svm_pipe = Pipeline([
    ("ct", ct),
    ("svm", svm.SVC(kernel="poly"))
])

param_grid = {
    "svm__degree": [2, 3, 4],
    "svm__C": [0.1, 1, 10],
    "svm__gamma": ["scale", "auto"], 
    "svm__coef0": [0.0, 0.5, 1.0]
}

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

svm_grid = GridSearchCV(
    svm_pipe, 
    param_grid=param_grid, 
    scoring="f1_macro", 
    cv=cv, 
    n_jobs=-1
)

svm_grid.fit(X_train, y_train)

# 9) results
print("Best parameters:", svm_grid.best_params_)
print("Best CV macro-F1: {:.3f}".format(svm_grid.best_score_))

# 10) test evaluation
y_pred = svm_grid.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

cm = confusion_matrix(y_test, y_pred, labels=["indica", "sativa"])
print("\nConfusion Matrix:\n", cm)
```
Best parameters: {'svm__C': 0.1, 'svm__coef0': 1.0, 'svm__degree': 4, 'svm__gamma': 'scale'}

Best performing SVM used a polynomial kernel with regularization to prevent overfitting. 

Best CV macro-F1: 0.848
The F1-score is higher than the SVC model showing that it outperformed SVC and QDA but still is slightly below LDA.

Classification Report:
               precision    recall  f1-score   support

      indica      0.872     0.932     0.901       132
      sativa      0.877     0.780     0.826        82

    accuracy                          0.874       214
   macro avg      0.875     0.856     0.863       214
weighted avg      0.874     0.874     0.872       214

Indica had a recall of .932 and Sativa has a recall of .78. 

Confusion Matrix:
 [[123   9] --> out of 132 predictions 123 were correct and 9 were misclassified
 [ 18  64]] --> out of 82 predictions, 64 were correct and 18 were misclassified

For the binary classification task, the dataset was limited to cannabis strains labeled as Sativa or Indica. All models used a consistent preprocessing pipeline that scaled numeric features and one-hot encoded categorical variables through a ColumnTransformer. Model performance was evaluated using macro- F1-score, which balances performance across both classes without favoring one over the other.

Among these, LDA model which had the highest overall F1-Score, demonstrating strong balance between precision and recall for both Sativa and Indica classifications. Its confusion matrix showed accurate predictions for most observations, indicating effective generalization after hyperparameter tuning. 


Part Two: Natural Multiclass
Now use the full dataset, including the Hybrid strains. 

Q1: Fit a decision tree, plot the final fit, and interpret the results.
```{python}
from sklearn.tree import DecisionTreeClassifier, plot_tree

X = df.drop("Type", axis=1)
y = df["Type"]

num_cols = X.select_dtypes(include=[np.number]).columns
cat_cols = X.select_dtypes(exclude=[np.number]).columns

ct = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols)
])

dt_pipe = Pipeline([
    ("ct", ct),
    ("tree", DecisionTreeClassifier(random_state=42))
])

param_grid = {
    "tree__max_depth": [3, 5, 7, 10, None],
    "tree__min_samples_split": [2, 5, 10]
}

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

dt_grid = GridSearchCV(
    dt_pipe, 
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv, 
    n_jobs=-1
)

dt_grid.fit(X_train, y_train)

print("Best parameters:", dt_grid.best_params_)
print("Best CV macro-F1:{:.3f}".format(dt_grid.best_score_))

y_pred = dt_grid.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))


```
Best parameters: {'tree__max_depth': 3, 'tree__min_samples_split': 2}
A decision tree with depth=3 performed the best. This means the dataset didn't support too complex split and deeper tree would overfit. 

Best CV macro-F1:0.602
The F1-score of 0.602 shows moderate performance for the average across all three classes equally.

Classification Report:
               precision    recall  f1-score   support

      hybrid      0.605     0.667     0.634       225
      indica      0.600     0.636     0.618       132
      sativa      0.431     0.268     0.331        82

    accuracy                          0.583       439
   macro avg      0.545     0.524     0.528       439
weighted avg      0.571     0.583     0.573       439

Hybrid had the best performance among the three classes with a recall value of .667 and this makes sense because Hybrid has a large sample size and lies between Indica and Sativa.
Indica performs similar to Hybrid, which can be because of the distinct effects of "relaxed" or "sleepy" which the decision tree can pick up. 
Sativa had the lowest recall of 0.268 and this can be because Sativa shares effects with many Hybrid of "energetic" and "happy"

The overall accuracy is about 58% which is much worse than the Part 1 binary models, and this is likely due to when adding the Hybrid class because the classes become less distinguishable. 

Confusion Matrix:
 [[150  47  28] --> many hybrids were correctly classified but misclassified 47 Indica and 28 Sativa. 
 [ 47  84   1] --> Indica was mostly correctly classified but misclassified with hybrid
 [ 51   9  22]] --> only 22 Sativa correctly classified and 51 misclassified as hybrid and 9 as Indica 

Overall decision tree performance was low of 58%, with Hybrid and Indica predicted somewhat accurately but Sativa performed poorly. It makes sense because Hybrid strains share similar effects with Sativa and Indica and Sativa could overlap with hybrid. The confusion matrix showed some mixing up between Sativa and Hybrid to show that the decision boundaries were not distinguishable for these two categories.


```{python}
import matplotlib.pyplot as plt
best_tree = dt_grid.best_estimator_.named_steps["tree"]
plt.figure(figsize=(15,8))
plot_tree(best_tree, filled=True, max_depth=3, fontsize=8)
plt.show()
```
The decision tree divided the strains based on flavor and effect indicators. For hybrids, they appear in many branches, but they are getting mixed up with Indica in some leaves. It is hard for the tree to separate better, and this matches the low recall score for Sativa in the metrics. For Indica, it is grouped on the left side of the tree and is labeled with effects of relaxed, sleepy, earthy. This is easier to classify than Sativa and was consistent with the model performance. Sativa was mostly found on the right branches and were separated based on strong/unique "uplifting" features. There was some overlap with hybrid so there was some mix up. 


Q2: Report the analyses from Part One for LDA, QDA, and KNN
```{python}
lda_full_pipe = Pipeline([
    ("ct", ct),
    ("lda", LDA())
])

param_grid = [
    {"lda__solver": ["lsqr", "eigen"], "lda__shrinkage": ["auto", 0.1, 0.3, 0.5, 0.7, 0.9]},
]

lda_full_grid = GridSearchCV(
    lda_full_pipe,
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv,
    n_jobs = -1
)

lda_full_grid.fit(X_train, y_train)

print("LDA Best Params:", lda_full_grid.best_params_)
print("LDA CV macro-F1: {:.3f}".format(lda_full_grid.best_score_))
print(classification_report(y_test, lda_full_grid.predict(X_test), digits=3))
print(confusion_matrix(y_test, lda_full_grid.predict(X_test)))
```
LDA Best Params: {'lda__shrinkage': 0.5, 'lda__solver': 'lsqr'}
LDA CV macro-F1: 0.616
              precision    recall  f1-score   support

      hybrid      0.629     0.564     0.595       225
      indica      0.580     0.689     0.630       132
      sativa      0.438     0.427     0.432        82

    accuracy                          0.576       439
   macro avg      0.549     0.560     0.552       439
weighted avg      0.578     0.576     0.575       439

[[127  57  41]
 [ 37  91   4]
 [ 38   9  35]]


QDA
```{python}
qda_full_pipe = Pipeline([
    ("ct", ct),
    ("qda", QDA())
])

param_grid = {"qda__reg_param": [0.0,0.2, 0.5, 0.8, 0.9, 0.99]}

qda_full_grid = GridSearchCV(
    qda_full_pipe,
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv,
    n_jobs=-1
)

qda_full_grid.fit(X_train, y_train)

print("QDA Best Params:", qda_full_grid.best_params_)
print("QDA CV macro-F1:{:.3f}".format(qda_full_grid.best_score_))
print(classification_report(y_test, qda_full_grid.predict(X_test), digits=3))
print(confusion_matrix(y_test, qda_full_grid.predict(X_test)))
```
QDA Best Params: {'qda__reg_param': 0.0}
QDA CV macro-F1: nan
              precision    recall  f1-score   support

      hybrid      0.545     0.293     0.382       225
      indica      0.412     0.530     0.464       132
      sativa      0.270     0.488     0.348        82

    accuracy                          0.401       439
   macro avg      0.409     0.437     0.398       439
weighted avg      0.454     0.401     0.400       439

[[66 83 76]
 [30 70 32]
 [25 17 40]]

KNN
```{python}
from sklearn.neighbors import KNeighborsClassifier

knn_full_pipe = Pipeline([
    ("ct", ct),
    ("knn", KNeighborsClassifier())
])

param_grid = {
    "knn__n_neighbors": [3, 5, 7, 9, 11],
    "knn__weights": ["uniform", "distance"]
}

knn_full_grid = GridSearchCV(
    knn_full_pipe,
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv,
    n_jobs=-1
)

knn_full_grid.fit(X_train, y_train)

print("KNN Best Params:", knn_full_grid.best_params_)
print("KNN CV macro-F1:{:.3f}".format(knn_full_grid.best_score_))
print(classification_report(y_test, knn_full_grid.predict(X_test), digits=3))
print(confusion_matrix(y_test, knn_full_grid.predict(X_test)))
```
KNN Best Params: {'knn__n_neighbors': 7, 'knn__weights': 'distance'}
KNN CV macro-F1: 0.511
              precision    recall  f1-score   support

      hybrid      0.571     0.698     0.628       225
      indica      0.563     0.508     0.534       132
      sativa      0.444     0.244     0.315        82

    accuracy                          0.556       439
   macro avg      0.526     0.483     0.492       439
weighted avg      0.545     0.556     0.541       439

[[157  44  24]
 [ 64  67   1]
 [ 54   8  20]]

Q3: Were your metrics better or worse than in Part One? Why? Which categories were most likely to get mixed up, according to the confusion matrices? Why?

Across all three models, the metrics in part Two were worse than the metrics in Part One. In part one, only binary classification was used between Indica and Sativa and both classes were distinguishable. In part two, the dataset included Hybrid strains too and this made the classifying the strains much more difficult. This is mostly because the hybrid category share the effects of flavor characteristics with Indica and Sativa. This means that hybrid strains will resemble Indicas and other can resemble Sativas. This overlap can make it difficult for LDA and QDA to find the boundaries. Because of this confusion, this caused all three models to have lower F1 score, accuracy, and recall compared to the binary results in Part One. 
Adding a third class increased confusion because now every sample now has two incorrect classes it could be misclassified into, which increases the chances of confusion. Across all confusion matrices, the most misclassified was Sativa, which were predicted as Hybrid and sometimes predicted as Indica. Sativa was rarely predicted correctly compared to the other two classes. Hybrid was the second most confused class which had the highest number of samples and got predicted as Indica and sometimes as Sativa. Indica was the easiest class to identify and across models it scored fairly higher than the others. In LDA, it had a recall of 0.69, 0.53 for QDA, and 0.51 for KNN. 

Part Three: Multiclass from Binary
Consider two models designed for binary classification: SVC and Logistic Regression

Q1: Fit and report metrics for OvR versions of the models. That is, for each of the two model types, create three models:
- Indica vs. Not Indica
- Sativa vs. Not Sativa
- Hybrid vs. Not Hybrid

Indica vs. Not-Indica 
```{python}
y_train_indica = (y_train == "indica").astype(int)
y_test_indica = (y_test == "indica").astype(int)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# logistic regression (OvR)
lr_indica = Pipeline([
    ("ct", ct),
    ("lr", LogisticRegression())
])

print("LR (Indica vs. Not) - CV F1:", cross_val_score(lr_indica, X_train, y_train_indica, scoring="f1", cv=cv).mean()
)

lr_indica.fit(X_train, y_train_indica)
pred_lr = lr_indica.predict(X_test)
print("\nLR Test Report (Indica vs. Not):\n", classification_report(y_test_indica, pred_lr, digits=3))
print("LR Confusion Matrix:\n", confusion_matrix(y_test_indica, pred_lr))

# SVC (poly) OvR
svc_indica = Pipeline([
    ("ct", ct),
    ("svc", SVC(kernel="poly"))
])

print("\nSVC (poly) (Indica vs. Not) - CV F1:", cross_val_score(svc_indica, X_train, y_train_indica, scoring="f1", cv=cv).mean())

svc_indica.fit(X_train, y_train_indica)
pred_svc = svc_indica.predict(X_test)

print("\nSVC Test Report (Indica vs. Not):\n", classification_report(y_test_indica, pred_svc, digits=3))
print("SVC Confusion Matrix:\n", confusion_matrix(y_test_indica, pred_svc))
```
LR (Indica vs. Not) - CV F1: 0.6398129647895296

LR Test Report (Indica vs. Not):
               precision    recall  f1-score   support

           0      0.825     0.847     0.836       307
           1      0.621     0.583     0.602       132

    accuracy                          0.768       439
   macro avg      0.723     0.715     0.719       439
weighted avg      0.764     0.768     0.766       439

LR Confusion Matrix:
 [[260  47]
 [ 55  77]]

SVC (poly) (Indica vs. Not) - CV F1: 0.2175791648609064

SVC Test Report (Indica vs. Not):
               precision    recall  f1-score   support

           0      0.708     0.925     0.802       307
           1      0.395     0.114     0.176       132

    accuracy                          0.681       439
   macro avg      0.551     0.519     0.489       439
weighted avg      0.614     0.681     0.614       439

SVC Confusion Matrix:
 [[284  23]
 [117  15]]

Sativa vs. Not-Sativa
```{python}
y_train_sativa = (y_train == "sativa").astype(int)
y_test_sativa = (y_test == "sativa").astype(int)

# logistic regression (OvR)
lr_sativa = Pipeline([
    ("ct", ct),
    ("lr", LogisticRegression())
])

print("LR (Sativa vs. Not) - CV F1:", cross_val_score(lr_sativa, X_train, y_train_sativa, scoring="f1", cv=cv).mean())

lr_sativa.fit(X_train, y_train_sativa)
pred_lr = lr_sativa.predict(X_test)
print("\nLR Test Report (Sativa vs. Not):\n", classification_report(y_test_sativa, pred_lr, digits=3))
print("LR Confusion Matrix:\n", confusion_matrix(y_test_sativa, pred_lr))

# SVC (poly) OvR
svc_sativa = Pipeline([
    ("ct", ct),
    ("svc", SVC(kernel="poly"))
])

print("\nSVC (poly) (Sativa vs. Not) - CV F1:", cross_val_score(svc_sativa, X_train, y_train_sativa, scoring="f1", cv=cv).mean())

svc_sativa.fit(X_train, y_train_sativa)
pred_svc = svc_sativa.predict(X_test)

print("\nSVC Test Report (Sativa vs. Not):\n", classification_report(y_test_sativa, pred_svc, digits=3))
print("SVC Confusion Matrix:\n", confusion_matrix(y_test_sativa, pred_svc))
```
LR (Sativa vs. Not) - CV F1: 0.4188886588788212

LR Test Report (Sativa vs. Not):
               precision    recall  f1-score   support

           0      0.851     0.947     0.897       357
           1      0.548     0.280     0.371        82

    accuracy                          0.822       439
   macro avg      0.700     0.614     0.634       439
weighted avg      0.795     0.822     0.798       439

LR Confusion Matrix:
 [[338  19]
 [ 59  23]]

SVC (poly) (Sativa vs. Not) - CV F1: 0.12864670281014162

SVC Test Report (Sativa vs. Not):
               precision    recall  f1-score   support

           0      0.817     0.989     0.895       357
           1      0.429     0.037     0.067        82

    accuracy                          0.811       439
   macro avg      0.623     0.513     0.481       439
weighted avg      0.745     0.811     0.740       439

SVC Confusion Matrix:
 [[353   4]
 [ 79   3]]

Hybrid vs. Not Hybrid
```{python}
y_train_hybrid = (y_train == "hybrid").astype(int)
y_test_hybrid = (y_test == "hybrid").astype(int)

# logistic regression (OvR)
lr_hybrid = Pipeline([
    ("ct", ct), 
    ("lr", LogisticRegression())
])

print("LR (Hybrid vs. Not) - CV F1:", cross_val_score(lr_hybrid, X_train, y_train_hybrid, scoring="f1", cv=cv).mean())

lr_hybrid.fit(X_train, y_train_hybrid)
pred_lr = lr_hybrid.predict(X_test)
print("\nLR Test Report (Hybrid vs. Not):\n", classification_report(y_test_hybrid, pred_lr, digits=3))
print("LR Confusion Matrix:\n", confusion_matrix(y_test_hybrid, pred_lr))

# SVC (poly) OvR
svc_hybrid = Pipeline([
    ("ct", ct),
    ("svc", SVC(kernel="poly"))
])
print("\nSVC (poly) (Hybrid vs. Not) - CV F1:", cross_val_score(svc_hybrid, X_train, y_train_hybrid, scoring="f1", cv=cv).mean())

svc_hybrid.fit(X_train, y_train_hybrid)
pred_svc = svc_hybrid.predict(X_test)
print("\nSVC Test Report (Hybrid vs. Not):\n", classification_report(y_test_hybrid, pred_svc, digits=3))
print("SVC Confusion Matrix:\n", confusion_matrix(y_test_hybrid, pred_svc))
```
LR (Hybrid vs. Not) - CV F1: 0.6470795687046913

LR Test Report (Hybrid vs. Not):
               precision    recall  f1-score   support

           0      0.619     0.584     0.601       214
           1      0.624     0.658     0.641       225

    accuracy                          0.622       439
   macro avg      0.622     0.621     0.621       439
weighted avg      0.622     0.622     0.621       439

LR Confusion Matrix:
 [[125  89]
 [ 77 148]]

SVC (poly) (Hybrid vs. Not) - CV F1: 0.6581620123175869

SVC Test Report (Hybrid vs. Not):
               precision    recall  f1-score   support

           0      0.552     0.248     0.342       214
           1      0.531     0.809     0.641       225

    accuracy                          0.535       439
   macro avg      0.541     0.528     0.491       439
weighted avg      0.541     0.535     0.495       439

SVC Confusion Matrix:
 [[ 53 161]
 [ 43 182]]

Q2: Which of the six models did the best job distinguishing the target category from the rest? Which did the worst? Does this make intuitive sense?

Indica vs. Not Indica:
Best model = Logistic Regression 
CV F1 Score = 0.64 
This model had the highest F1 score overall; balanced precision/recall (0.60 F1 on test)

Sativa vs. Not Sativa:
Best model = Logistic Regression 
CV F1 Score = 0.42 

Hybrid vs. Not Hybrid:
Best Model = SVC (poly)
CV F1 Score = 0.66 
SVC slightly beat LR (0.65); Hybrid class has more overlap

Best performing: Indica vs. Not Indica (Logistic Regression)
Indica has a clearer, more distinct chemical and sensory characteristics.
"Relaxed", "Sleepy", "Earthy" that separate it from the others 

Worst performing: Sativa vs. Not Sativa (SVC and LogReg both have low F1)
Sative and Hybrid share overlapping features ("Energetic", "Happy"), making it hard for either model to find a clean boundary. Hybrid models had moderate results: both algorithms captured some structure but confused Hybrids with Indicas due to mixed traits. 

Indica strains are more distinct, so even simple linear models (LogReg) can separate them, so it does make sense. Sativa and Hybrid strains overlap heavily, so both linear (LogReg) and polynomial (SVC) models struggled. SVC's polynomial kernel helps a bit for Hybrid beause nonlinear interactions capture mixed "Indica + Sativa" characteristics. 

Q3: Fit and report metrics for OvO versions of the models. That is, for each of the two model types, create three models:

- Indica vs. Sativa
- Indica vs. Hybrid
- Hybrid vs. Sativa

Indica vs. Sativa
```{python}
# filter for sativa and indica 
X_pair = X[y.isin(["indica", "sativa"])]
y_pair = y[y.isin(["indica", "sativa"])]

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# logistic regression
lr_pipe = Pipeline([
    ("ct", ct),
    ("lr", LogisticRegression())
])

auc_lr = cross_val_score(lr_pipe, X_pair, y_pair, scoring="roc_auc", cv=cv).mean()
print("LogReg (Indica vs. Sativa) ROC-AUC:", auc_lr)

# SVC (poly)
svc_pipe = Pipeline([
    ("ct", ct),
    ("svc", SVC(kernel="poly"))
])
auc_svc = cross_val_score(svc_pipe, X_pair, y_pair, scoring="roc_auc", cv=cv).mean()
print("SVC (poly) (Indica vs. Sativa) ROC-AUC:", auc_svc)
```
LogReg (Indica vs. Sativa) ROC-AUC: 0.9263228699832684
SVC (poly) (Indica vs. Sativa) ROC-AUC: 0.8760026950514963

Indica vs. Hybrid 
```{python}
X_pair = X[y.isin(["indica", "hybrid"])]
y_pair = y[y.isin(["indica", "hybrid"])]

auc_lr = cross_val_score(lr_pipe, X_pair, y_pair, scoring="roc_auc_ovr", cv=cv).mean()
auc_svc = cross_val_score(svc_pipe, X_pair, y_pair, scoring="roc_auc_ovr", cv=cv).mean()

print("LogReg (Indica vs Hybrid) ROC-AUC:", auc_lr)
print("SVC (poly) (Indica vs. Hybrid) ROC-AUC:", auc_svc)
```
LogReg (Indica vs Hybrid) ROC-AUC: 0.7910503689659224
SVC (poly) (Indica vs. Hybrid) ROC-AUC: 0.8760026950514963

Hybrid vs. Sativa
```{python}
X_pair = X[y.isin(["hybrid", "sativa"])]
y_pair = y[y.isin(["hybrid", "sativa"])]

auc_lr = cross_val_score(lr_pipe, X_pair, y_pair, scoring="roc_auc", cv=cv).mean()
auc_svc = cross_val_score(svc_pipe, X_pair, y_pair, scoring="roc_auc", cv=cv).mean()

print("LogReg (Hybrid vs Sativa) ROC-AUC:", auc_lr)
print("SVC (poly) (Hybrid vs. Sativa) ROC-AUC:", auc_svc)
```
LogReg (Hybrid vs Sativa) ROC-AUC: 0.7359636512238138
SVC (poly) (Hybrid vs. Sativa) ROC-AUC: 0.6732759609364172

Q4: Which of the six models did the best job distinguishing at differentiating the two groups? Which did the worst? Does this make intuitive sense?

Best Overall Model:
Logisitic Regression for Indica vs. Sativa
The Logistic Regression ROC-AUC score for the Indica vs. Sativa model was the highest with a score of 0.926 of all the six model. This makes sense because Indica and Sativa are the most distinguishable cannabis strains. They have different effects, flavors, and feelings which cluster more better so a linear classifier can separate them well. 

Worst Model:
The worst overall model was the SVC for Hybrid vs. Sativa, with a ROC-AUC score of 0.673 which was the lowest of the six. This was consistent with the other model results from part 2. Hybrid and Sativa were the hardest pair due to overlap in their effect/feeling, both sharing similar descriptors and because of this similarity, the SVC polynomial boundary wasn't enough to distinguish them. 

Q5: Suppose you had simply input the full data, with three classes, into the LogisticRegression function. Would this have automatically taken an "OvO" approach or an "OvR" approach? What about SVC?

If all three classes were inputted into LogisticRegression, this would automatically taken an OvR approach to take one classifier per class vs. the rest. This is what was ran manually in Q1 where we classified Indica vs. Not Indica, Sativa vs. Not Sativa, Hybrid vs. Not Hybrid. By default, LogisticRegression uses OvR for multiclass which means if you fit the Logistic Regression model, the three binary classifiers will be trained. I did this manually in part 3 where I created labels for the strain and fit the LogReg model for Indica vs. Not Indica etc. Part 1 Q1 results showed what LogReg would have done automatically. 
SVC uses OvO by default for multiclass classification, which means if you fit the model with the three classes it automatically trains classifiers in pairs with Indica vs. Sativa and etc. This was manually done in OvO in Part 3 Q3 where I created subsets for each pair and calculated the ROC-AUC for both Logistic Regression and SVC. The OvO process matched with SVC's scikit learn process
